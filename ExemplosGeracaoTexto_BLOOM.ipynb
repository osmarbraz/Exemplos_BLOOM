{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMKsZs+LcCYe3sY+5dbCIUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osmarbraz/exemplos_BLOOM/blob/main/ExemplosGeracaoTexto_BLOOM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "#Exemplo de Geração de textos usando BLOOM Transformers by HuggingFace\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/bloom\n",
        "\n",
        "https://huggingface.co/blog/bloom-megatron-deepspeed\n",
        "\n",
        "https://huggingface.co/blog/zero-shot-eval-on-the-hub\n",
        "\n",
        "\n",
        "# **A execução pode ser feita através do menu Ambiente de Execução opção Executar tudo.**\n",
        "\n",
        "Exemplo de geração de textos em:\n",
        "- https://huggingface.co/spaces/huggingface/bloom_demo\n",
        "- https://studyexperts.in/blog/how-to-use-the-bloom-model-using-python/\n",
        "\n",
        "\n",
        "Treinamento:\n",
        "- Hardware\t384 80GB A100 GPUs\n",
        "- Software\tMegatron-DeepSpeed\n",
        "- Arquitetura GPT3 w/ extras\n",
        "- Conjunto de dados de 350 Bilhões de tokens de 59 linguas\n",
        "- Tempo de treinamento\t3.5 meses\n",
        "\n",
        "**Link biblioteca Huggingface:**\n",
        "https://github.com/huggingface/transformers\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxb5Px3p1-e"
      },
      "source": [
        "# 0 - Preparação do ambiente\n",
        "Preparação do ambiente para execução do exemplo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAPVtRXQqDim"
      },
      "source": [
        "## Tratamento de logs\n",
        "\n",
        "Método para tratamento dos logs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcopxbGZqDip"
      },
      "source": [
        "# Biblioteca de logging\n",
        "import logging\n",
        "\n",
        "# Formatando a mensagem de logging\n",
        "logging.basicConfig(format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjYtXcMnSAe"
      },
      "source": [
        "## Identificando o ambiente Colab\n",
        "\n",
        "Cria uma variável para identificar que o notebook está sendo executado no Google Colaboratory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMiH0E3OnRa1"
      },
      "source": [
        "# Se estiver executando no Google Colaboratory\n",
        "import sys\n",
        "\n",
        "# Retorna true ou false se estiver no Google Colaboratory\n",
        "IN_COLAB = \"google.colab\" in sys.modules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqa-7WXBAw8q"
      },
      "source": [
        "# 1 - Instalação Transformer da Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCdqJCtQN52l"
      },
      "source": [
        "Instala a interface pytorch para o BERT by Hugging Face. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RfUN_KolV-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2795d3e-82ae-4e3c-d87b-d08ddbd15d9c"
      },
      "source": [
        "# Instala a última versão da biblioteca\n",
        "# !pip install transformers\n",
        "\n",
        "# Instala uma versão específica da biblioteca\n",
        "!pip install -U transformers==4.23.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.23.1\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 48.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (5.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.23.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.23.1) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.23.1) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.23.1) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.23.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.23.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.23.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.23.1) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcpd9t9PpkrX"
      },
      "source": [
        "# 2 - Carregando o BLOOM\n",
        "\n",
        "O modelo BLOOM foi proposto com suas várias versões através do BigScience Workshop. A BigScience é inspirada em outras iniciativas de ciência aberta onde os pesquisadores reuniram seu tempo e recursos para alcançar coletivamente um impacto maior. A arquitetura do BLOOM é essencialmente semelhante ao GPT3 (modelo auto-regressivo para previsão do próximo token), mas foi treinado em 46 linguagens diferentes e 13 linguagens de programação. Várias versões menores dos modelos foram treinadas no mesmo conjunto de dados. BLOOM está disponível nas seguintes versões:\n",
        "\n",
        "\n",
        "Modelos:\n",
        " - bigscience-small-testing (32,3 MB) - Ok Colab\n",
        " - bloom-560m (1,12 GB) - Ok Colab \n",
        " - bloom-1b1 (2,13 GB) - Ok Colab\n",
        " - bloom-1b7 (3,44 GB) - Não carrega no Colab\n",
        " - bloom-3b (6,01 GB) - Não carrega no Colab\n",
        " - bloom-7b1 (9,98 + 4,16 GB) - Não carrega no Colab\n",
        " - bloom (176 Bilhões de parâmetros) (7,19 GB + 44 * 4,93GB) = 224,11 GB - Não carrega no Colab\n",
        "\n",
        "\n",
        "Lista dos modelos:\n",
        "  - https://huggingface.co/models?other=bloom"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# modelo_bloom = \"bigscience-small-testing\"\n",
        "# modelo_bloom = \"bigscience/bloom-560m\"\n",
        "modelo_bloom = \"bigscience/bloom-1b1\"\n",
        "\n",
        "# modelo_bloom = \"bigscience/bloom-1b7\"\n",
        "# modelo_bloom = \"bigscience/bloom-3b\"\n",
        "# modelo_bloom = \"bigscience/bloom-7b1\"\n",
        "# modelo_bloom = \"bigscience/bloom\""
      ],
      "metadata": {
        "id": "8VyTD85uYnau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Carrega o tokenizador"
      ],
      "metadata": {
        "id": "qbEwi-sdjUEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_bloom)"
      ],
      "metadata": {
        "id": "EmHTB0FMEAJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "17a0cc6709c646ff818c55f2bc1e9501",
            "77f924a129e042fe96e65c3051e181e6",
            "78d51378f48d47198889d6b39707eb01",
            "5bad78b745404bc3a697057df323c56d",
            "069d61a80385400088c556cefa36b7b7",
            "b69a4b65f8e945348f96dae5c2cb003a",
            "c16cc2e27ccd4d64bfbeddbff58a10a3",
            "0d4f1d08fc2c438f921c43d231725229",
            "edecaf1f515f4d54b9f929a708bc3969",
            "a4968bbba80e453994a6e1d6df6f3a6d",
            "d73dd12356c34dc7809d0cb411a3df40",
            "fc9b80f31553464e8c2b42e5bb593d07",
            "9baf9af090aa467abbd22c8df1535559",
            "065651903198444d9d15488174268b8d",
            "5d7bab6d022b4299928d27c381b90420",
            "3e7841744e3b4996a0426c2d4364def3",
            "a727037c9ee84e379f7b26ab623acf11",
            "8e55aa45e8254eb78cd3959c76730d80",
            "96c7e6dc844a45eab6aee18cf98359e5",
            "ca0a1059be61485e8960aa9014367071",
            "3baffd396aaa47b6aa74b1571da8aad0",
            "097d048b1a954af2b529cc855760420e",
            "4079ecf47e4244f6816d3d90b7f36a1a",
            "16e8b8b7a9ba4ca3ba41e25b901f61c7",
            "120c566cd633479c929cde6af5908436",
            "045d9d24dd6e433da858f474c5da0a1a",
            "cef649e054444742a2b9e14b65971991",
            "66125fb1ec5e4c2382ce4746e8542f2a",
            "fc85fe7de41643c49cd537d95ed36343",
            "17cc6357f55b4d74a8e91776e96bab40",
            "5f7469858a29470188574a6120dab5ce",
            "fccdb39d63f84ff2a35c0ded5a3698b1",
            "122c095ff7e84dcb8fcf60c248ffacb8"
          ]
        },
        "outputId": "d96f4170-691a-4df6-f941-fbfd830f6469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17a0cc6709c646ff818c55f2bc1e9501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc9b80f31553464e8c2b42e5bb593d07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4079ecf47e4244f6816d3d90b7f36a1a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__On2g1a--K"
      },
      "source": [
        "## 2.2 - Carregando o Modelo BLOOM(AutoModelForCausalLM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(modelo_bloom, use_cache=True)"
      ],
      "metadata": {
        "id": "13pciB5xEJ_T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3008b954a45f4c47aae3660ed03a4aab",
            "446591e0dda147d988635f4ab415d706",
            "b1a11d3179c7479b818c6a0ffbe6f10a",
            "941ad39d18cd4a09a95dbbfce0867117",
            "2c0f8aa0434e4850bb2a69889ba79796",
            "2c0658966c6a41ce9459d3b44085aaeb",
            "ffa5d51291a146bbb40d421d676c2055",
            "7a83165266f84af29299ba1cab0d4330",
            "bfa1df1fa1df4a8aa0b56e331d851d4d",
            "47e46802167a4eb1977a6b9e4d10a714",
            "468428b90d8d46da900b981113b52016",
            "8a16f78faae54f72a8aa393219904265",
            "eea045f9dd254ac3ad739ebcb34e2d20",
            "2c467e59cc914e329f0f7c695eadd32f",
            "8bd9707d071442a083a6087faa6bc2a8",
            "26b8b3fc57ee4a6db85cf14953ad105b",
            "f87990b57d134ddc96643d1b18391b37",
            "0960a0c0e0f940fbb906ca1fe383fa5b",
            "cccb82f511344f2dbec62de50c1039f1",
            "201a69d2ea32469c90b7bd51435c8141",
            "f4897864e3b04eaebfbc3c2d43e51678",
            "fd65533d15304494934dc64ee03f4676"
          ]
        },
        "outputId": "474beb6a-24e3-40f9-a4dd-6e92ffbc3f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/688 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3008b954a45f4c47aae3660ed03a4aab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a16f78faae54f72a8aa393219904265"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Exemplos de geração de textos com BLOOM"
      ],
      "metadata": {
        "id": "1qezcBkxnEdR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Pd6-h0YD8U"
      },
      "source": [
        "## 3.1 - Exemplo geração de texto\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QP-2tC8YOFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e0424c-3631-4944-c658-5e5aa37ed8e4"
      },
      "source": [
        "# Define o documento base\n",
        "# documento = \"Como empilhar elementos em muma pilha?\"\n",
        "#documento = \"How to push elements in a stack\"\n",
        "#documento = \"O comando SQL para extrair todos os usuários cujo nome começa com A é:\"\n",
        "documento = \"Bom dia professor, tudo bem ?\"\n",
        "# documento = \"The SQL command to extract all the users whose name starts with A is:\"\n",
        "#documento = \"How to push elements in a stack\"\n",
        "#documento = \"Write code for finding the prime number in python ?\"\n",
        "# documento = \"Escrever código para encontrar o número primo em python?\"\n",
        "\n",
        "# Se pt for especificado, ele retornará tensores em vez de lista de inteiros python e tokenizará os documentos\n",
        "input = tokenizer(documento, return_tensors=\"pt\")\n",
        "\n",
        "# Mostra os tokens com seus índices\n",
        "i = 0\n",
        "for tup in zip(input['input_ids'][0], input['attention_mask'][0]):\n",
        "    # print(tup[0], tup[1])\n",
        "    print(\"{:>3} {} {} {}\".format(i, tokenizer.convert_ids_to_tokens(tup[0].item()), tup[0], tup[1]))\n",
        "    i= i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0 Bom 82152 1\n",
            "  1 Ġdia 2515 1\n",
            "  2 Ġprofessor 26804 1\n",
            "  3 , 15 1\n",
            "  4 Ġtudo 11845 1\n",
            "  5 Ġbem 7658 1\n",
            "  6 Ġ? 2040 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6RqhuI0Ys51"
      },
      "source": [
        "# Ele obterá o resultado e podemos fornecer o número máximo de palavras como parâmetro\n",
        "# podemos reduzir o plágio ajustando o valor da temperatura mais próximo de 1\n",
        "\n",
        "exemplo = model.generate(**input, max_length=100, top_k=0, temperature=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostra o resultado\n",
        "print(tokenizer.decode(exemplo[0]))\n",
        "\n",
        "# print(tokenizer.decode(exemplo.squeeze(), skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBjGgvwsX9XT",
        "outputId": "0bf295fc-0d05-4bb5-e281-c90f9c419b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bom dia professor, tudo bem?Estou com uma dúvida, eu fiz um projeto de um sistema de controle de acesso, e o sistema de controle de acesso é feito com o sistema de controle de acesso de um computador, e o sistema de controle de acesso é feito com o sistema de controle de acesso de um computador, e o sistema de controle de acesso é feito com o sistema de controle de acesso de um computador, e o sistema de controle de acesso é feito com o sistema de controle de acesso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 - Outro exemplo\n",
        "\n",
        "https://pythontechworld.com/issue/huggingface/transformers/18809"
      ],
      "metadata": {
        "id": "02tbUrQNn90D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_gen_loss(lm_logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "    batch_size = labels.shape[0]\n",
        "    shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "    shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "    loss = loss_fct(\n",
        "        shift_logits.view(-1, shift_logits.size(-1)),\n",
        "        shift_labels.view(-1)\n",
        "    )\n",
        "    loss = loss.reshape(batch_size, -1)\n",
        "    loss = loss.sum(dim=-1) / (shift_labels != -100).sum(dim=-1)\n",
        "    \n",
        "    return loss"
      ],
      "metadata": {
        "id": "I_eDGUAboBu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_ids(arrays, padding, max_length=-1):\n",
        "    if (max_length < 0):\n",
        "        max_length = max(list(map(len, arrays)))\n",
        "\n",
        "    arrays = [[padding] * (max_length - len(array)) + array for array in arrays]\n",
        "\n",
        "    return arrays"
      ],
      "metadata": {
        "id": "3IyjJ6P3oCcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(text: list, labels: str, conditional: bool = True):\n",
        "\n",
        "    input_tokens = tokenizer(text).input_ids\n",
        "    label_tokens = tokenizer(labels).input_ids\n",
        "\n",
        "    input_ids = [x + y for (x, y) in zip(input_tokens, label_tokens)]\n",
        "    attention_mask = [(len(x) + len(y)) * [1] for (x, y) in zip(input_tokens, label_tokens)]\n",
        "    if (conditional):\n",
        "        labels = [[-100] * len(x) + y for (x, y) in zip(input_tokens, label_tokens)]\n",
        "    else:\n",
        "        labels = input_ids\n",
        "\n",
        "    pad = 3\n",
        "    input_ids = pad_ids(input_ids, pad)\n",
        "    attention_mask = pad_ids(attention_mask, 0)    \n",
        "    # rótulos precisam estar no dispositivo de saída\n",
        "    labels = pad_ids(labels, -100)\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    labels = torch.tensor(labels)\n",
        "    \n",
        "    # submete ao modelo\n",
        "    lm_logits = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    ).logits\n",
        "\n",
        "    # Mostra o resultado\n",
        "    print(compute_gen_loss(lm_logits, labels).cpu().tolist())\n"
      ],
      "metadata": {
        "id": "Nzble8JToEqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "    # \"DeepSpeed\",\n",
        "    # \"DeepSpeed is a\",\n",
        "    # \"DeepSpeed is a machine\",\n",
        "    # \"DeepSpeed is a machine learning framework\",\n",
        "    \"Deep Speed\",\n",
        "    \"DeepSpeed é um\",\n",
        "    \"DeepSpeed é uma máquina\",\n",
        "    \"DeepSpeed é uma estrutura de aprendizado de máquina\",\n",
        "]\n",
        "labels = [\n",
        "    # \" is awesome.\",\n",
        "    # \" good person.\",\n",
        "    # \" that can wipe out the planet.\",\n",
        "    # \" for generating memes.\",\n",
        "    \" é incrível.\",\n",
        "    \" boa pessoa.\",\n",
        "    \" que pode acabar com o planeta.\",\n",
        "    \" para gerar memes.\",\n",
        "]\n",
        "\n",
        "print(\"Original\")\n",
        "forward(text, labels)\n",
        "\n",
        "# labels[0] = \" is awesome. really awesome\"\n",
        "labels[0] = \" é incrível. realmente incrível\"\n",
        "forward(text, labels)\n",
        "\n",
        "#labels[0] = \" is awesome. really awesome. Try it.\"\n",
        "labels[0] = \" é incrível. realmente incrível. Experimente.\"\n",
        "forward(text, labels)\n",
        "\n",
        "# labels[0] = \" is awesome. really awesome. Try it. You'll be surprised\"\n",
        "labels[0] = \" é incrível. realmente incrível. Experimente. Você ficará surpreso\"\n",
        "forward(text, labels)\n",
        "\n",
        "# labels[0] = \" is awesome. really awesome. Try it. You'll be surprised. BLOOM was trained using DeepSpeed.\"\n",
        "labels[0] = \" é incrível. realmente incrível. Experimente. Você ficará surpreso. BLOOM foi treinado usando DeepSpeed.\"\n",
        "forward(text, labels)\n",
        "\n",
        "# labels[0] = \" is awesome. really awesome. Try it. You'll be surprised. BLOOM was trained using DeepSpeed. Oh no the values are bugging out now.\"\n",
        "labels[0] = \" é incrível. realmente incrível. Experimente. Você ficará surpreso. O BLOOM foi treinado usando DeepSpeed. Ah, não, os valores estão bugados agora.\"\n",
        "forward(text, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Z7x4PDn_0_",
        "outputId": "7141f1bb-c6b7-4d32-a778-6db09dc5e0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original\n",
            "[5.990021228790283, 7.568333148956299, 3.1083452701568604, 4.449141979217529]\n",
            "[6.825961112976074, 7.568333148956299, 3.1083452701568604, 4.449141979217529]\n",
            "[5.244286060333252, 7.568333148956299, 3.1083452701568604, 4.449141979217529]\n",
            "[4.190319538116455, 7.568333148956299, 3.108344793319702, 4.449139595031738]\n",
            "[4.509304523468018, 7.568332672119141, 3.108344554901123, 4.449138641357422]\n",
            "[4.370351791381836, 7.568332195281982, 3.1083462238311768, 4.4491400718688965]\n"
          ]
        }
      ]
    }
  ]
}